ssh://nirvana@0df579c1e9b76a66.natapp.cc:2051/home/nirvana/hfai_envs/ssl_0/bin/python -u /home/nirvana/workspace/PycharmProjects/Semi-supervised-learning/train.py --c config/mlp_ssl_tiny/debug.yaml
Is local debug, reset attributes.
/home/nirvana/workspace/PycharmProjects/Semi-supervised-learning/train.py:54: UserWarning: You have chosen to seed training. This will turn on the CUDNN deterministic setting, which can slow down your training considerably! You may see unexpected behavior when restarting from checkpoints.
  warnings.warn('You have chosen to seed training. '
[2023-04-28 00:39:54,014 INFO] Use GPU: 0 for training
Files already downloaded and verified
lb count: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]
ulb count: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]
Files already downloaded and verified
[2023-04-28 00:39:55,287 INFO] unlabeled data number: 50000, labeled data number 40
[2023-04-28 00:39:56,323 INFO] Create train and test data loaders
[2023-04-28 00:39:56,324 INFO] [!] data loader keys: dict_keys(['train_lb', 'train_ulb', 'eval'])
[2023-04-28 00:39:56,391 INFO] Create optimizer and scheduler
[2023-04-28 00:39:56,392 INFO] Number of Trainable Params: 1469771
[2023-04-28 00:39:56,413 INFO] Arguments: Namespace(T=0.5, algorithm='mlp_fixmatch', amp=False, backbone_temperature_scaling=1.0, batch_size=64, c='config/mlp_ssl_tiny/debug.yaml', calc_weight_method='mlp', class_mismatch_ratio=0, clip=0.0, clip_grad=0, crop_ratio=0.875, data_dir='./data', datadiet_adjust_lr_decay=False, datadiet_exp_version=0, datadiet_grad_params='backbone', datadiet_influence_calculate_num=1, datadiet_influence_group_size=448, datadiet_interval=1073741824, datadiet_keep_num=1073741824, datadiet_method=None, datadiet_val_grad_method='mixup', datadiet_warmup_epoch=40, dataset='cifar10', disable_shm=False, dist_backend='nccl', dist_url='tcp://127.0.0.1:29958', distributed=True, ema_m=0.999, epoch=1000, eval_batch_size=256, gpu=0, hard_label=True, imb_algorithm=None, img_size=32, l2_weight=0.2, layer_decay=1.0, lb_dest_len=40, lb_imb_ratio=1, load_path='None', lr=0.03, max_length=512, max_length_seconds=4.0, momentum=0.9, multiprocessing_distributed=True, net='mlp_wrn_28_2', net_from_name=False, num_classes=10, num_eval_iter=1000, num_labels=40, num_log_iter=10, num_train_iter=300000, num_warmup_iter=0, num_workers=0, optim='SGD', overwrite=True, p_cutoff=0.95, pretrain_path='', rank=0, resume=False, sample_rate=16000, save_dir='./saved_models/debug', save_interval=1000000000000, save_name='mlp_fixmatch_cifar10_40_4gpu', seed=0, train_sampler='RandomSampler', ulb_dest_len=50000, ulb_imb_ratio=1, ulb_loss_ratio=1.0, ulb_num_labels=None, uratio=7, use_amp=False, use_cat=True, use_pretrain=False, use_tensorboard=True, vcc_datapoint_bank_size=100, vcc_dec_model='early_fusion', vcc_dec_norm='none', vcc_decoder_dims=[256, 128], vcc_detach_input=False, vcc_disable_entropy=False, vcc_disable_temporal=False, vcc_disable_variance=False, vcc_disable_view=False, vcc_enc_norm='none', vcc_encoder_dims=[128, 256], vcc_lab_loss_weight=0.0, vcc_mc_dropsize=5, vcc_mc_keep_p=0.5, vcc_mc_sampling_times=20, vcc_mc_upd_ratio=1.0, vcc_only_supervised=False, vcc_p_cutoff=0.95, vcc_recon_loss='cross_entropy', vcc_reinit_threshold=False, vcc_selection_warmup=1073741824, vcc_training_warmup=1073741824, vcc_uncertainty_method='mcdropout', vcc_unlab_kl_loss_weight=0.0, vcc_unlab_recon_loss_weight=0.0, vcc_variance_warmup=0, vcc_z_dim=0, visualize_eval_results_path=None, visualize_load_path=None, weight_decay=0.0005, world_size=1)
[2023-04-28 00:39:56,413 INFO] Resume load path None does not exist
[2023-04-28 00:39:56,413 INFO] Model training
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[2023-04-28 00:39:58,961 INFO] model saved: ./saved_models/debug/mlp_fixmatch_cifar10_40_4gpu/model_best.pth
[2023-04-28 00:40:04,807 INFO] 10 iteration, USE_EMA: True, {'train/sup_loss': 1.3348690271377563, 'train/unsup_loss': 0.4912525713443756, 'train/select_loss': 0.11340905725955963, 'train/total_loss': 1.939530611038208, 'train/mask_ratio': 0.4329548180103302, 'lr': 0.029999999968514934, 'train/prefecth_time': 0.43223907470703127, 'train/run_time': 0.2156094970703125}
[2023-04-28 00:40:11,274 INFO] 20 iteration, USE_EMA: True, {'train/sup_loss': 0.6209217309951782, 'train/unsup_loss': 0.7760226130485535, 'train/select_loss': 0.12326382845640182, 'train/total_loss': 1.5202081203460693, 'train/mask_ratio': 0.38368090987205505, 'lr': 0.029999999874059734, 'train/prefecth_time': 0.4331602478027344, 'train/run_time': 0.21456358337402343}
[2023-04-28 00:40:17,741 INFO] 30 iteration, USE_EMA: True, {'train/sup_loss': 0.15142042934894562, 'train/unsup_loss': 0.6263516545295715, 'train/select_loss': 0.13544881343841553, 'train/total_loss': 0.9132208824157715, 'train/mask_ratio': 0.32275596261024475, 'lr': 0.029999999716634404, 'train/prefecth_time': 0.43109194946289064, 'train/run_time': 0.21569033813476562}
[2023-04-28 00:40:24,283 INFO] 40 iteration, USE_EMA: True, {'train/sup_loss': 0.11099504679441452, 'train/unsup_loss': 0.5270956158638, 'train/select_loss': 0.1461760252714157, 'train/total_loss': 0.7842667102813721, 'train/mask_ratio': 0.26911991834640503, 'lr': 0.02999999949623894, 'train/prefecth_time': 0.43527801513671877, 'train/run_time': 0.21696774291992188}
[2023-04-28 00:40:30,783 INFO] 50 iteration, USE_EMA: True, {'train/sup_loss': 0.04053211957216263, 'train/unsup_loss': 0.3817647397518158, 'train/select_loss': 0.15625227987766266, 'train/total_loss': 0.5785491466522217, 'train/mask_ratio': 0.2187386304140091, 'lr': 0.029999999212873347, 'train/prefecth_time': 0.4322984313964844, 'train/run_time': 0.21681610107421875}
[2023-04-28 00:40:37,291 INFO] 60 iteration, USE_EMA: True, {'train/sup_loss': 0.01836126483976841, 'train/unsup_loss': 0.30323371291160583, 'train/select_loss': 0.16701672971248627, 'train/total_loss': 0.48861169815063477, 'train/mask_ratio': 0.16491644084453583, 'lr': 0.029999998866537626, 'train/prefecth_time': 0.43304690551757813, 'train/run_time': 0.21611465454101564}
[2023-04-28 00:40:43,799 INFO] 70 iteration, USE_EMA: True, {'train/sup_loss': 0.01630902662873268, 'train/unsup_loss': 0.24235720932483673, 'train/select_loss': 0.17215999960899353, 'train/total_loss': 0.43082624673843384, 'train/mask_ratio': 0.13920006155967712, 'lr': 0.029999998457231773, 'train/prefecth_time': 0.43216958618164064, 'train/run_time': 0.21850297546386718}
[2023-04-28 00:40:50,294 INFO] 80 iteration, USE_EMA: True, {'train/sup_loss': 0.01215773168951273, 'train/unsup_loss': 0.17250239849090576, 'train/select_loss': 0.17899787425994873, 'train/total_loss': 0.36365801095962524, 'train/mask_ratio': 0.10501059889793396, 'lr': 0.02999999798495579, 'train/prefecth_time': 0.4318242492675781, 'train/run_time': 0.21537667846679687}
[2023-04-28 00:40:56,806 INFO] 90 iteration, USE_EMA: True, {'train/sup_loss': 0.013939732685685158, 'train/unsup_loss': 0.1534365564584732, 'train/select_loss': 0.18134470283985138, 'train/total_loss': 0.3487209975719452, 'train/mask_ratio': 0.09327651560306549, 'lr': 0.02999999744970968, 'train/prefecth_time': 0.435600341796875, 'train/run_time': 0.21646185302734375}
[2023-04-28 00:41:03,320 INFO] 100 iteration, USE_EMA: True, {'train/sup_loss': 0.007439692039042711, 'train/unsup_loss': 0.11562307178974152, 'train/select_loss': 0.18454475700855255, 'train/total_loss': 0.3076075315475464, 'train/mask_ratio': 0.0772763043642044, 'lr': 0.02999999685149344, 'train/prefecth_time': 0.43393048095703124, 'train/run_time': 0.21848626708984376}
[2023-04-28 00:41:09,831 INFO] 110 iteration, USE_EMA: True, {'train/sup_loss': 0.005365424323827028, 'train/unsup_loss': 0.0987517461180687, 'train/select_loss': 0.18755914270877838, 'train/total_loss': 0.2916763126850128, 'train/mask_ratio': 0.06220439076423645, 'lr': 0.029999996190307077, 'train/prefecth_time': 0.4305003662109375, 'train/run_time': 0.21756483459472656}
[2023-04-28 00:41:16,346 INFO] 120 iteration, USE_EMA: True, {'train/sup_loss': 0.004900642670691013, 'train/unsup_loss': 0.09362412989139557, 'train/select_loss': 0.18833564221858978, 'train/total_loss': 0.2868604063987732, 'train/mask_ratio': 0.05832182243466377, 'lr': 0.029999995466150592, 'train/prefecth_time': 0.4340050048828125, 'train/run_time': 0.21872840881347655}
[2023-04-28 00:41:22,863 INFO] 130 iteration, USE_EMA: True, {'train/sup_loss': 0.004925931338220835, 'train/unsup_loss': 0.08267418295145035, 'train/select_loss': 0.1901838630437851, 'train/total_loss': 0.27778398990631104, 'train/mask_ratio': 0.049080751836299896, 'lr': 0.02999999467902398, 'train/prefecth_time': 0.4360896301269531, 'train/run_time': 0.21644154357910156}
[2023-04-28 00:41:29,356 INFO] 140 iteration, USE_EMA: True, {'train/sup_loss': 0.00397478137165308, 'train/unsup_loss': 0.07061082124710083, 'train/select_loss': 0.19113023579120636, 'train/total_loss': 0.2657158374786377, 'train/mask_ratio': 0.04434885457158089, 'lr': 0.02999999382892725, 'train/prefecth_time': 0.4320343322753906, 'train/run_time': 0.21656031799316405}
[2023-04-28 00:41:35,874 INFO] 150 iteration, USE_EMA: True, {'train/sup_loss': 0.0037136373575776815, 'train/unsup_loss': 0.07288975268602371, 'train/select_loss': 0.19088540971279144, 'train/total_loss': 0.2674888074398041, 'train/mask_ratio': 0.04557303339242935, 'lr': 0.0299999929158604, 'train/prefecth_time': 0.4322220458984375, 'train/run_time': 0.21928361511230468}
[2023-04-28 00:41:42,393 INFO] 160 iteration, USE_EMA: True, {'train/sup_loss': 0.003108824137598276, 'train/unsup_loss': 0.06719302386045456, 'train/select_loss': 0.19104504585266113, 'train/total_loss': 0.26134687662124634, 'train/mask_ratio': 0.04477478936314583, 'lr': 0.02999999193982343, 'train/prefecth_time': 0.43527706909179686, 'train/run_time': 0.21772268676757814}
